{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import uniform\n",
    "from fairlearn.metrics import *\n",
    "from sklearn.metrics import *\n",
    "import scipy.stats.distributions as dists\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from fairmlhealth import report, measure\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_4564/2106370369.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\kevin\\AppData\\Local\\Temp/ipykernel_4564/2106370369.py\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    race_Asian-Pac-Islander\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "path = 'data/census/adult-preprocessed.csv'\n",
    "file_dataframe = pd.read_csv(path, delimiter=',')\n",
    "\n",
    "# Columns\n",
    "CATEGORICAL_COLUMNS = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "BIAS_COLUMNS = [\n",
    "    'age', 'race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Black',\n",
    "    'race_Other', 'race_White', 'sex_Female', 'sex_Male']\n",
    "LABEL_COLUMN = \"income\"\n",
    "NUMERICAL_COLUMNS = ['capital-loss', 'capital-gain', 'age', 'fnlwgt', 'education-num', 'hours-per-week']\n",
    "IGNORE_COLUMNS = ['is_train']\n",
    "\n",
    "categorical_columns_df = file_dataframe[CATEGORICAL_COLUMNS]\n",
    "file_dataframe.drop(columns=CATEGORICAL_COLUMNS, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(dataframe):\n",
    "    scaler = MinMaxScaler()\n",
    "    for i in NUMERICAL_COLUMNS:\n",
    "        dataframe[i] = scaler.fit_transform(dataframe[[i]])\n",
    "    return dataframe\n",
    "\n",
    "# def standard_scaling(dataframe):\n",
    "#     scaler = StandardScaler()\n",
    "#     for i in NUMERICAL_COLUMNS:\n",
    "#         dataframe[i] = scaler.fit_transform(dataframe[[i]])\n",
    "#     return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_classifiers(output_dir, X, y, X_test, y_test):\n",
    "    # Calculate negative positive ratio\n",
    "    negative_positive_ratio = sum(file_dataframe[LABEL_COLUMN] == 0) / sum(file_dataframe[LABEL_COLUMN] == 1)\n",
    "\n",
    "    # Define the classifiers\n",
    "    classifiers = [\n",
    "        SVC(max_iter=1000, C=1e9, class_weight=\"balanced\"),#SGDClassifier(loss='log', alpha=0.01, max_iter=2000, tol=0, class_weight='balanced'),s\n",
    "        RandomForestClassifier(class_weight=\"balanced\"),\n",
    "        XGBClassifier(objective='binary:logistic', scale_pos_weight=negative_positive_ratio, use_label_encoder=False, seed=seed, eval_metric='auc')\n",
    "        ]\n",
    "\n",
    "    # Define hyperparameters for the classifiers. \n",
    "    # Note: I did not use hyperparameters since performance is not a focus of the excersise as mentioned by the professor\n",
    "    hyperparameters = [\n",
    "        dict(C= uniform(0.1, 100), gamma=['scale', 'auto'], kernel=['linear', 'rbf']),\n",
    "        dict(bootstrap= [True, False], max_features= ['auto', 'sqrt'], max_depth=dists.randint(5, 50), n_estimators= dists.randint(50, 200), min_samples_leaf= [1, 2, 4], min_samples_split= [2, 5, 10]),\n",
    "        dict(eta=[0.001, 0.1, 0.3, 0.5], min_child_weight=[1, 3, 5], max_depth=dists.randint(5, 50), n_estimators=  dists.randint(50, 200), subsample= [0.5, 0.8, 1.0])\n",
    "    ]\n",
    "\n",
    "    # Define the crosss validation strategy\n",
    "    # cv = KFold(n_splits=10)\n",
    "    best_auc, best_y_pred = 0, None\n",
    "\n",
    "    # Iterate all classifiers\n",
    "    for i in range(len(classifiers)):\n",
    "        # Select the classifier and its hyperparameters for the experimentation\n",
    "        classifier = classifiers[i]\n",
    "        hp = hyperparameters[i]\n",
    "\n",
    "        # Define the hyperparameter search strategy and find the best model accordingly\n",
    "        clf = RandomizedSearchCV(classifier, hp, n_iter=30, scoring ='roc_auc', n_jobs=-1, verbose=0)\n",
    "        clf.fit(X, y)\n",
    "        best_model = clf.best_estimator_\n",
    "\n",
    "        # Predict the labels given the test set's features\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        with open(f'{output_dir}/{best_model.__class__.__name__}.npy', 'wb') as f:\n",
    "            np.save(f, y_pred)\n",
    "        \n",
    "        # Evaluate the performance of the model based on the test set\n",
    "        performance_metrics(output_dir, i, y_test, y_pred)\n",
    "\n",
    "        fairmlhealth_metrics(best_model, X_test, y_test, y_pred)\n",
    "\n",
    "        if best_auc < auc:\n",
    "            best_y_pred = y_pred\n",
    "            best_auc = auc\n",
    "    return best_y_pred, best_auc\n",
    "\n",
    "def performance_metrics(output_dir, classifier_id, y_test, y_predictions):\n",
    "    classifier_names = ['SVC', 'RandomForestClassifier', 'XGBClassifier']\n",
    "    with open(f'{output_dir}/{classifier_names[classifier_id]}_results.txt', 'w') as f:\n",
    "        print(\"----- PERFORMANCE METRICS -----\", file=f)\n",
    "        print(\"--- ACCURACY SCORE ---\", file=f)\n",
    "        print(accuracy_score(y_test, y_predictions), file=f)\n",
    "        print(\"--- PRECISION SCORE ---\", file=f)\n",
    "        print(precision_score(y_test, y_predictions, average='weighted'), file=f)\n",
    "        print(\"--- RECALL SCORE ---\", file=f)\n",
    "        print(recall_score(y_test, y_predictions, average='weighted'), file=f)\n",
    "        print(\"--- F1 SCORE ---\", file=f)\n",
    "        print(f1_score(y_test, y_predictions, average='weighted'), file=f)\n",
    "        print(\"--- CLASSIFICATION REPORT ---\", file=f)\n",
    "        print(classification_report(y_test, y_predictions), file=f)\n",
    "        print(\"--- CONFUSION MATRIX ---\", file=f)\n",
    "        print(confusion_matrix(y_test, y_predictions), file=f)\n",
    "        false_pos_rate, true_pos_rate, thresholds = roc_curve(y_test, y_predictions, pos_label=1)\n",
    "        print(\"--- FALSE POSITIVE RATE ---\", file=f)\n",
    "        print(false_pos_rate, file=f)\n",
    "        print(\"--- TRUE POSITIVE RATE ---\", file=f)\n",
    "        print(true_pos_rate, file=f)\n",
    "        print(\"--- AREA UNDER CURVE ---\", file=f)\n",
    "        print(auc(false_pos_rate, true_pos_rate), file=f)\n",
    "        print(\"--- ROC AUC ---\", file=f)\n",
    "        print(roc_auc_score(y_test, y_predictions), file=f)\n",
    "        \n",
    "def fairmlhealth_metrics(model, X_test, y_test, y_pred):\n",
    "    for bias_feature in BIAS_COLUMNS:\n",
    "        report.compare(test_data=X_test, targets=y_test, predictions=y_pred, protected_attr=X_test[bias_feature], models=model)\n",
    "    measure.performance(X=X_test, y_true=y_test, y_pred=y_pred, features=BIAS_COLUMNS)\n",
    "    measure.bias(X_test[BIAS_COLUMNS], y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotOccurence(data,colname,label):\n",
    "    plot=pd.crosstab(index=data[colname],columns=data[label]).plot(kind='bar',stacked=True,figsize=(16,5))\n",
    "    plt.xlabel(colname)\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(axis='y',linestyle='-')\n",
    "    plt.title(colname+\" vs \"+label+\" count\")\n",
    "\n",
    "def plotProportion(data,colname,label):\n",
    "    plot=pd.crosstab(index=data[colname],columns=data[label],normalize='index').plot(kind='bar',stacked=True,figsize=(16,5))\n",
    "    plt.xlabel(colname)\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.grid(axis='y',linestyle='-')\n",
    "    plt.title(colname+\" vs \"+label+\" proportion\")\n",
    "\n",
    "def draw_fairlearn_figure(y_test, y_pred, data):\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score,\n",
    "        'f1': f1_score,\n",
    "        'precision': precision_score,\n",
    "        'recall': recall_score,\n",
    "        'false positive rate': false_positive_rate,\n",
    "        'true positive rate': true_positive_rate,\n",
    "        'selection rate': selection_rate,\n",
    "        'count': count,\n",
    "        'demographic parity difference': demographic_parity_difference,\n",
    "        'demographic parity ratio': demographic_parity_ratio,\n",
    "        'equalized odds difference': equalized_odds_difference,\n",
    "        'equalized odds ratio': equalized_odds_ratio}\n",
    "    metric_frame = MetricFrame(metrics=metrics,\n",
    "                            y_true=y_test,\n",
    "                            y_pred=y_pred,\n",
    "                            sensitive_features=data)\n",
    "    metric_frame.by_group.plot.bar(\n",
    "        subplots=True,\n",
    "        layout=[4, 2],\n",
    "        legend=False,\n",
    "        figsize=[12, 8],\n",
    "        title=\"Show all metrics\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test\n",
    "train = file_dataframe[file_dataframe.is_train==True].drop(columns=['is_train'])\n",
    "test = file_dataframe[file_dataframe.is_train==False].drop(columns=['is_train'])\n",
    "\n",
    "train = min_max_scaling(train)\n",
    "test = min_max_scaling(test)\n",
    "\n",
    "# train = standard_scaling(train)\n",
    "# test = standard_scaling(test)\n",
    "\n",
    "# Prepare train set's features and labels\n",
    "# X = train.drop(columns=[LABEL_COLUMN]).to_numpy()\n",
    "# y = train[LABEL_COLUMN].to_numpy().astype(bool)\n",
    "X = train.drop(columns=[LABEL_COLUMN])\n",
    "y = train[LABEL_COLUMN].astype(bool)\n",
    "\n",
    "# Prepare test set's features and labels\n",
    "# X_test = test.drop(columns=[LABEL_COLUMN]).to_numpy()\n",
    "# y_test = test[LABEL_COLUMN].to_numpy().astype(bool)\n",
    "X_test = test.drop(columns=[LABEL_COLUMN])\n",
    "y_test = test[LABEL_COLUMN].astype(bool)\n",
    "\n",
    "best_y_pred, best_auc = predict_all_classifiers('outputs/census', X, y, X_test, y_test)\n",
    "\n",
    "y_pred = best_y_pred\n",
    "PRED_COLUMN = 'predicted '+ LABEL_COLUMN\n",
    "preds = pd.DataFrame(y_pred, columns=[PRED_COLUMN])\n",
    "\n",
    "file_dataframe = pd.concat([file_dataframe, categorical_columns_df], axis=1)\n",
    "\n",
    "gold_data = file_dataframe[file_dataframe.is_train==False]\n",
    "gold_data.reset_index(drop=True, inplace=True)\n",
    "gold_data[LABEL_COLUMN].replace({False: '<=50K', True: '>50K'}, inplace=True)\n",
    "gold_data.age = gold_data.age // 10 * 10\n",
    "\n",
    "pred_data = pd.concat([gold_data, preds], axis=1)\n",
    "pred_data[PRED_COLUMN].replace({False: '<=50K', True: '>50K'}, inplace=True)\n",
    "\n",
    "for col in BIAS_COLUMNS:\n",
    "    plotOccurence(gold_data, col, LABEL_COLUMN)\n",
    "    plotOccurence(pred_data, col, PRED_COLUMN)\n",
    "    plotProportion(gold_data, col, LABEL_COLUMN)\n",
    "    plotProportion(pred_data, col, PRED_COLUMN)\n",
    "    draw_fairlearn_figure(y_test, y_pred, gold_data[col])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2afc3b7ee2fb652a9160709bc7e3e62fe7209a9e9efd42aa830a9fd06b5f7b76"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
